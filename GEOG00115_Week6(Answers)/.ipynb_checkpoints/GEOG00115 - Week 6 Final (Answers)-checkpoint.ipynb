{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### GEOG0115\n",
    "\n",
    "Introduction to Social and Geographic Data Science\n",
    "-----------------------------------\n",
    "\n",
    "Computer Lab 6\n",
    "-------------------------------\n",
    "\n",
    "Note: Notebook might contain scripts and instructions adapted from GEOG0115, GEOG0051. \n",
    "Contributors: Stephen Law, Mateo Neira, Nikki Tanu and Thomas Keel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Lab Notebook 6.1: Introduction to Machine Learning in SGDS II\n",
    "-------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Lab Notebook 6.1: Introduction to Machine Learning in SGDS II\n",
    "-------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boston House Price dataset\n",
    "\n",
    "The historic Boston Housing Price dataset (Harrison & Rubenfield, 1978) that we use was taken from the [StatLib library which is maintained at Carnegie Mellon University](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/) and is freely available for download from the UCI Machine Learning Repository. The dataset consists of 506 observations of 13 attributes (1 attribute had to be removed). **The median value of house price in $10000s**, denoted by `MEDV`, is the target feature in our model. Below is a brief description of each feature and the outcome in our dataset: Variables:\n",
    "\n",
    "* `CRIM` â€“ per capita crime rate by town\n",
    "\n",
    "* `ZN` â€“ proportion of residential land zoned for lots over 25,000 sq.ft\n",
    "\n",
    "* `CHAS` â€“ Charles River dummy variable (1 if tract bounds river; else 0) \n",
    "\n",
    "* `NOX` â€“ nitric oxides concentration (parts per 10 million)\n",
    "\n",
    "* `RM` â€“ average number of rooms per dwelling\n",
    "\n",
    "* `AGE` â€“ proportion of owner-occupied units built prior to 1940\n",
    "\n",
    "* `DIS` â€“ weighted distances to five Boston employment centres\n",
    "\n",
    "* `RAD` â€“ index of accessibility to radial highways\n",
    "\n",
    "* `INDUS` â€“ proportion of non-retail business acres per town\n",
    "\n",
    "* `TAX` â€“ full-value property-tax rate per 10,000\n",
    "\n",
    "* `PTRATIO` â€“ pupil-teacher ratio by town\n",
    "\n",
    "* `LSTAT` â€“ lower income proportions of the population\n",
    "\n",
    "* `MEDV` â€“ Median value of owner-occupied homes in 10000\n",
    "\n",
    "As you will already be familiar with, we have to first load in the dataset into Jupyter Notebook. The method of importation is, however, different as the dataset is already included as part of the `sklearn` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error,r2_score, mean_absolute_error\n",
    "\n",
    "import seaborn as sns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "df=pd.read_csv('BostonHousing_adj.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# set X and y\n",
    "X=df[df.columns[df.columns!='MEDV']] #same as df.drop(columns='MEDV')\n",
    "y=df['MEDV']\n",
    "\n",
    "print (X.shape)\n",
    "print (y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describing the dataset\n",
    " \n",
    "Having taken the precautionary steps above, we can then proceed to look at the data contained within the imported data frames. Fundamentally, we may want to explore the distributions of some important variables (columns) within their individual vectors. Beyond that, depending on your application, correlations between these variables might be something pertinent to describe the data with, or running simple diagnostics to determine if and how missingness in the data should be dealt with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plots the distribution of the X variables to assess whether they are appropriate for regression\n",
    "X.hist(bins=20, # determines how thick each 'partition' of the data (bars in each histogram) is \n",
    "       figsize=(10,10), color='lightblue', xlabelsize=0, ylabelsize=0, grid=False\n",
    "      ) # note how we can directly call the method `.hist()` because `X` is a pd.data.frame\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming the dataset\n",
    "\n",
    "What do you notice from the histograms above? \n",
    "\n",
    "It is apparent that most of the input variables are continuous (except for ```CHAS``` which is a dummy variable) and most do not follow a symmetric distribution or resembling a normal distribution. The truth is that in more datasets than not that you will be working with data that are skewed in one way or another. One common strategy is to transform predictor, particularly those that are more heavily skewed, before running a regression model. \n",
    "\n",
    "The main motivation for this is to approximate linearity and to reduce the potential impact of skewed distributions, which could lead to biased or inefficient estimates in the regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pulls out only numeric data columns - in case there were any non-numeric ones\n",
    "numeric_data = X.select_dtypes(include=[np.number])\n",
    "\n",
    "# this removes the charles river dummy variable\n",
    "numeric_data.drop('CHAS', inplace=True, axis=1)\n",
    "#inplace=True modifies the DataFrame in place, meaning it removes the 'CHAS' column\n",
    "#directly in the original numeric_data df without needing to assign the result to a new variable\n",
    "\n",
    "# imports function to check whether a variable is over a given amount of skewedness\n",
    "from scipy.stats import skew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we use the `skew` function below within an `.apply()` method of Pandas data frames, which allows us to easily reiterate an operation over rows/columns of a data frame (in this case removing any `NA`s and then once again converting the vectors to the type `float` to ensure the calculation of skewness proceeds smoothly.\n",
    "\n",
    "The skewness values can be interpreted in the following manner and you could [read more here](https://www.pluralsight.com/guides/interpreting-data-using-descriptive-statistics-python):\n",
    "\n",
    "* Highly skewed distribution: If the skewness value is less than âˆ’1 (left) or greater than +1 (right).\n",
    "\n",
    "* Moderately skewed distribution: If the skewness value is between âˆ’1 (left) and âˆ’Â½ or between +Â½ and +1 (right).\n",
    "\n",
    "* Approximately symmetric distribution: If the skewness value is between âˆ’Â½ and +Â½.\n",
    "\n",
    "What do the values we have suggest? Refer back to the histograms of these variables that we had made earlier and consider how different values of skewness correspond to these distributions.\n",
    "\n",
    "![skewness](https://miro.medium.com/max/600/1*nj-Ch3AUFmkd0JUSOW_bTQ.jpeg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantifies the skewness of the numeric data columns and filters based on thresholds\n",
    "skewed = X[numeric_data.columns].apply(lambda x: skew(x.dropna().astype(float))) #lambda is used to create a generic function\n",
    "\n",
    "# Identify the right and left skewed variables (extract index)\n",
    "rskewed = skewed[skewed > 0.75].index\n",
    "lskewed = skewed[skewed < -0.75].index\n",
    "\n",
    "# Plot histograms for right skewed variables\n",
    "X[rskewed].hist(bins=20, figsize=(15, 7), color='lightblue', layout=(2, 4), grid=False)\n",
    "\n",
    "# Plot histograms for left skewed variables\n",
    "X[lskewed].hist(bins=20, figsize=(15, 7), color='orange', layout=(2, 4), grid=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#same as \n",
    "# Define the function\n",
    "def calculate_skew(column):\n",
    "    return skew(column.dropna().astype(float))\n",
    "\n",
    "# Apply the function to each column\n",
    "skewed = X[numeric_data.columns].apply(calculate_skew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log-transforms the highly right skewed variables of the dataset\n",
    "X[rskewed] = np.log1p(X[rskewed])\n",
    "\n",
    "#for left-skewed data, you need to reflect the values first before applying the log transformation (i.e. make them right-skewed)\n",
    "X[lskewed] = np.log1p(np.max(X[lskewed])-X[lskewed]) \n",
    "\n",
    "# now plot again the variables after their log transformation\n",
    "X[rskewed].hist(bins=20,figsize=(15,7), color='lightblue', xlabelsize=0, ylabelsize=0, grid=False, layout=(2,4))\n",
    "plt.show()\n",
    "\n",
    "# now plot again the variables after their log transformation\n",
    "X[lskewed].hist(bins=20,figsize=(15,7), color='orange', xlabelsize=0, ylabelsize=0, grid=False, layout=(2,4))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### note on transformation\n",
    "\n",
    "Here we apply a common log transformations on the rightly skewed variable and leftly skewed variable. Logarithm is a very typical transformation to reduce the skewness of a variable while preserving the differences. \n",
    "\n",
    "What do you notice different pre- and post-transformation for the variables above? For which variables does the log-transformation seem to make it less skewed? It is important to note, data transformations is common to make skewed variable less skewed but it must be applied very cautiously and it isn't a necessary condition for running regression models as you loses interpretability for the specific feature. \n",
    "\n",
    "Be sure to play around with the many different arguments that allow you to customise the look of your multi-grid histogram plots with the `.hist()` (and all the other functions you come to interface with). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations/Covariance between variables\n",
    "\n",
    "As well as for skewedness of individual variables, it is useful to check for correlations between variables that we use as predictors. Strongly correlated predictor variables may, for instance, indicate that two predictors are measuring similar real-world concepts. (In statistical inference, this can be checked by the **variation inflation factor**.)\n",
    "\n",
    "### correlation matrix\n",
    "We can use a correlation matrix to understand the association between variables. Variables are almost always correlated. Ideally, we want our input variables to not be totally correlated with one another. Significant correlation between our independent variables undermines the robustness of our model and interpretability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate correlation matrix between all variables. Defaults to Pearson's r correlation\n",
    "corr = X.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's make our plot a little nicer\n",
    "f, ax = plt.subplots(figsize=(10, 10))\n",
    "sns.heatmap(corr, center=0,cmap=plt.get_cmap('viridis'),\n",
    "            square=True, linewidths=.05, annot=True, vmin=-1, vmax=1,ax=ax) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# hierarchically-clustered heatmap of correlation matrix to understand the association between variables\n",
    "corr = X.corr()\n",
    "sns.clustermap(corr, center=0,cmap=plt.get_cmap('viridis'),\n",
    "            square=True, linewidths=.05, annot=True, vmin=-1, vmax=1) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler() #scaling the features so they have a mean of 0 and a standard deviation of 1\n",
    "scaled_X = pd.DataFrame(scaler.fit_transform(X),columns=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "scaled_X.boxplot(vert=False,figsize=(8,8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plots the distribution of the X variables to assess whether they are appropriate for regression\n",
    "scaled_X.hist(bins=20, # determines how thick each 'partition' of the data (bars in each histogram) is \n",
    "       figsize=(10,10), color='lightblue', xlabelsize=0, ylabelsize=0, grid=False\n",
    "      ) # note how we can directly call the method `.hist()` because `X` is a pd.data.frame\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spliting the dataset into a train and test set\n",
    "\n",
    "Before we run the linear model, as a recap, remember how it is common in most Machine Learning applications to split the dataset up into at least two groups - the `train` and `test` sets, so as to ensure a degree of robustness and generalsability in the models we estimate. By building the model using one dataset, `train`, and testing its accuracy of predictions with an 'unseen' dataset, `test`, we can calibrate our models based on a realistic criterion. One major risk of not doing this would be to overfit the model, which means that, because it becomes so attuned to predicting values in the dataset it was trained on, it becomes much less generalisable and quickly falters when any other dataset is ever so slightly different in its structure. \n",
    "\n",
    "Below, [`train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) will help us do this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "### split the dataset\n",
    "(X_train, X_test, y_train, y_test) = train_test_split(scaled_X, y, train_size=0.7, random_state=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 6.1.1 Linear regression model \n",
    "Given a dataset of $n$ observations, LinearRegression takes a set of input features and trains a linear model to predict the target which takes the general form\n",
    "$y_i=\\beta_0+\\beta_1x_{i1}+\\beta_2x_{i2}+...+\\beta_px_{ip}+\\epsilon_i, i=1,...,n$ observations, where $x_{i}$ are the set of inputs, $\\beta$ are the corresponding coefficients and $y_i$ is the target. \n",
    "\n",
    "For example, in a simple linear regression model, the input can be the years of education $x_i$ and the target can be the income $y_i$. The aim of the linear regression is thus to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the model $y_i-\\hat{y_i}$. Important to note the linear regression model assumes linearity and normality of errors. \n",
    "\n",
    "<img src=\"regression.png\" alt=\"Description\" width=\"400\" height=\"300\">\n",
    "\n",
    "More formally the objective is to miniminise the loss function $argmin_{\\beta} \\sum_i^n (y_i - \\hat{f}(x_i))^2$ where the weights $\\beta$ can be estimated analytically (Least Square solution) or iteratively (Gradient Descent). \n",
    "\n",
    "Least square (OLS)\n",
    "```from sklearn import linear_model```\n",
    "\n",
    "gradient descent \n",
    "```from sklearn.linear_model import SGDRegressor```\n",
    "\n",
    "https://scikit-learn.org/stable/modules/linear_model.html\n",
    "\n",
    "Please read for more informaiton:\n",
    "https://mlu-explain.github.io/linear-regression/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# define Linear Regression model (OLS)\n",
    "model=LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# fit model on training data\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# report accuracy score for linear regression model\n",
    "y_pred=model.predict(X_test)\n",
    "print ('RMSE: ',mean_squared_error(y_test,y_pred,squared=False))\n",
    "print ('MSE: ',mean_squared_error(y_test,y_pred))\n",
    "print ('R2: ', r2_score(y_test,y_pred)) #R2 of 0.x means that the model explains x% of the variance in the target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Importance (Explainability Methods)\n",
    "\n",
    "Feature importance is an explaination technique to inspect a model. \n",
    "There are different ways to measure feature importance in machine learning models. \n",
    "For a linear model, ones can simply show the beta coefficients of the model similar to statistical analysis. \n",
    "\n",
    "For a more expressive model, a **model agnostic** way to explain model is [permutation importance](https://scikit-learn.org/stable/modules/permutation_importance.html#permutation-importance). \n",
    "\n",
    "\n",
    "**AdvanceNote(optional)** \n",
    "There are many other explainability methods. Shapley values is one that are used to explain the contributions of individual features to the predictions that considers all possible combinations made by complex models like gradient boosting or neural networks. This method is particularly valuable when you need to make black-box models more transparent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# you can then easily plot out feature importance for variables in descending order\n",
    "coef = pd.DataFrame(model.coef_, index = X.columns)\n",
    "coef = coef.reset_index()\n",
    "coef.columns=['features','importance']\n",
    "coef['importance']=np.abs(coef['importance'])\n",
    "coef=coef.sort_values(by='importance',ascending=False)\n",
    "plt.figure(figsize=(6,6))\n",
    "\n",
    "sns.barplot(x='importance',y='features',data=coef,)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isolating problems in our model's prediction\n",
    "Sometimes we want to isolate where our model have failed, so we have some idea about the shortcomings of our model and also an indication of how we can supplment and improve its inputs.   \n",
    "\n",
    "Let's look for the residuals with relatively large values. *Remember that*, residuals are the differences between observed and predicted values of data. In this case, we may interpret residuals as outliers.\n",
    "\n",
    "*Remember also* that our data refers to the \"Median value of owner-occupied homes in 10000(Â£)\", so any large residuals indicate areas where we are not very good at predicting Median value in those rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_resid = y_test - y_pred\n",
    "sns.distplot(y_resid)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take all values that are more than 10 or less than -10\n",
    "y_resid_high = y_resid[y_resid>10].sort_values(ascending=False)\n",
    "y_resid_high\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_resid_low = y_resid[y_resid<-10]\n",
    "y_resid_low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at those values in the original data\n",
    "df.iloc[y_resid_high.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at those values in the original data\n",
    "df.iloc[y_resid_low.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity checks - Interpreting residuals\n",
    "**Open-ended question**: Is there anything characteristic about these rows?\n",
    "\n",
    "*Note that:* Often outliers indicate there is some other dependency or unaccounted for trend in our data (i.e. a spatial dependency). Sometimes we can include more variables, more data, do different feature engineering to reduce residuals. **BUT**  Although it may be hard to interpret here, when you use a dataset you are more familiar with or one with a spatial element (LSOA, borough, State, Country, etc.), you will be able to isolate outliers and say something about where your model is and is not effective. And this inaccuracy is also valid and important for broader scientific knowledge discovery.\n",
    "\n",
    "In general, it is always useful to take a look at the outliers as sanity checks. If the data is geographical, the results can be plotted as a residual map to see whether the outliers cluster together. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statsmodel - An alternative library to estimate Generalised Linear Model in Python\n",
    "\n",
    "**Statsmodel** is an alternative library for estimating generalised linear models such as linear regression in Python. The library is more focus on statistical inference rather than machine learning prediction (sklearn). It reports on many statistics and inferential tests on the data that is not being reported in sklearn. This includes individual ```confidence intervals``` and ```t-tests``` to test the statistical significance for each input feature and useful model comparison metrics such as ```AIC``` and ```BIC```. This class focuses more on machine learning methods but it is important to note that similar statistical inference can be estimated with using statsmodels. It is important that we did not resample the dataset but a similar procedure can be applied here when running the regression model using Statsmodel.  \n",
    "\n",
    "See intpretation of OLS summary here: https://www.geeksforgeeks.org/interpreting-the-results-of-linear-regression-using-ols-summary/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.regression\n",
    "# Set up Linear regression model\n",
    "ols = statsmodels.regression.linear_model.OLS(y, X)\n",
    "# fit model\n",
    "results = ols.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Summarise the model fit\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent Regressor\n",
    "\n",
    "\n",
    "SGD stands for Stochastic Gradient Descent which uses gradient descent to fit a regression in an iterativel manner. Figure below illustrates the gradient descent algorithm which is to update $\\beta$ until loss is minimised or when the model is best fitted. This algorithm is generic and can be applied to many machine learning modelling methods!\n",
    "\n",
    "<img src=\"gradient_descent02.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "\n",
    "The main parameter to tune in a gradient descent algorithm is the learning rate $\\alpha$ which dictates how fast to change the weights and the number of epochs or iterations. If the learning rate is set too high then the optimal solution might not be found while if the learning rate is set too low then the convergence will take a long time. \n",
    "\n",
    "\n",
    "```from sklearn.linear_model import SGDRegressor```\n",
    "\n",
    "```class sklearn.linear_model.SGDRegressor(loss='squared_loss', *, penalty='l2', alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=1000, tol=0.001, shuffle=True, verbose=0, epsilon=0.1, random_state=None, learning_rate='invscaling', eta0=0.01, power_t=0.25, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, warm_start=False, average=False)```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ðŸ¤¨ TASK\n",
    "Try replacing ```LinearRegressor``` with ```SGDRegressor```, try tuning the learning rate and number of epochs.  Take a look at the ```SGDRegressor``` class for more information about each of its hyper-parameters.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelSGD = SGDRegressor(learning_rate='invscaling', max_iter=1000)\n",
    "modelSGD.fit(X_train, y_train)\n",
    "# report accuracy score for linear regression model\n",
    "y_pred=modelSGD.predict(X_test)\n",
    "print ('RMSE: ',mean_squared_error(y_test,y_pred,squared=False))\n",
    "print ('MSE: ',mean_squared_error(y_test,y_pred))\n",
    "print ('R2: ', r2_score(y_test,y_pred)) #R2 of 0.x means that the model explains x% of the variance in the target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lasso Regression\n",
    "\n",
    "Lasso Regression is a regularised linear model that includes an additional $\\ell^1$ loss in order to estimate sparse coefficients. In another words it shrinks less important coefficients to enhance prediction accuracy and interpretability. The alpha parameter of the Lasso needs to be tuned. Lasso regression is a model-based feature selection method.\n",
    "\n",
    "<img src=\"feature_selection.png\" alt=\"Description\" width=\"400\" height=\"300\">\n",
    "\n",
    "```from sklearn import linear_model```\n",
    "\n",
    "```class sklearn.linear_model.Lasso(alpha=1.0, *, fit_intercept=True, normalize=False, precompute=False, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic')```\n",
    "\n",
    "https://scikit-learn.org/stable/modules/linear_model.html#lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "model2 = linear_model.Lasso(alpha=0.01)\n",
    "model2.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ðŸ¤¨ TASK\n",
    "Try tuning the $\\alpha$ parameter for the lasso regressor. What happens when you tune the $\\alpha$ very high? Plot out the feature importance here as a task and compare it with the linear regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=model2.predict(X_test)\n",
    "print ('RMSE: ',mean_squared_error(y_test,y_pred,squared=False))\n",
    "print ('MSE: ',mean_squared_error(y_test,y_pred))\n",
    "print ('R2: ', r2_score(y_test,y_pred)) #R2 of 0.x means that the model explains x% of the variance in the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef = pd.DataFrame(model2.coef_, index = X.columns)\n",
    "coef = coef.reset_index()\n",
    "coef.columns=['features','importance']\n",
    "coef['importance']=np.abs(coef['importance'])\n",
    "coef=coef.sort_values(by='importance',ascending=False)\n",
    "plt.figure(figsize=(6,6))\n",
    "\n",
    "sns.barplot(x='importance',y='features',data=coef,)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1.2 Tree-based Regression\n",
    "\n",
    "Decision Trees (DTs) is a supervised learning method used for classification or regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. As implied by the 'tree' in its name, the model can pass each observation that goes through it via multiple branches (/routes) of decisions to help it derive a final outcome for that given observation). \n",
    "\n",
    "<img src=\"decision_tree.png\" alt=\"Description\" width=\"400\" height=\"300\">\n",
    "\n",
    "\n",
    "Below is an indicative example where a simple decision tree (maxdepth=2) is used to find out which mode of transport will a person take to go to work; car? public transport? or walk? using n=30 observations. \n",
    "\n",
    "1. The first split was found where \n",
    "if distance<1.2km then walk;\n",
    "else public transport or car\n",
    "\n",
    "2. The second split was found where \n",
    "if person has driver license then mostly drive;\n",
    "else mostly public transport.\n",
    "\n",
    "\n",
    "Decision Tree aims to split a dataset based on some features optimally (evaluation metric) using the **CART algorithm** and iterates until loss is zero or if maximum depth of the tree reaches a threshold. \n",
    "\n",
    "\n",
    "Click for information on the [parameters of the decision tree function](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ðŸ¤¨ TASK\n",
    "Try replacing ```LinearRegressor``` with ```Decision Tree Regressor```, try tuning the different hyper-parameters of a decision tree. \n",
    "\n",
    "Please read the sklearn manual here to learn more about decision tree.  \n",
    "https://scikit-learn.org/stable/modules/tree.html#tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeltree = DecisionTreeRegressor(max_depth=6)\n",
    "modeltree.fit(X_train, y_train)\n",
    "# report accuracy score for linear regression model\n",
    "y_pred=modeltree.predict(X_test)\n",
    "print ('RMSE: ',mean_squared_error(y_test,y_pred,squared=False))\n",
    "print ('MSE: ',mean_squared_error(y_test,y_pred))\n",
    "print ('R2: ', r2_score(y_test,y_pred)) #R2 of 0.x means that the model explains x% of the variance in the target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Evaluation metrics - regression\n",
    "\n",
    "The coefficient of determination **R2** measures the proportion of variance of the target that can be explained by the input features. It provides an indication of goodness of fit and therefore a measure of how well unseen samples are likely to be predicted by the model.\n",
    "\n",
    "$ 1-\\sum \\frac{(y_i-\\hat{y_i})}{(y_i-\\bar{y_i})} $\n",
    "\n",
    "The Mean Squared Error **mse** measurse the averaged squared error between the observed and the predicted. Similarly it provides an indication of goodness of fit where the lower is better.\n",
    "\n",
    "$\\frac{1}{n} \\sum (y_i - \\hat{y_i})^2 $\n",
    "\n",
    "The Mean Absolute Error **mae** measures the average absolute error between the observed and the predicted. Similarly it provides an indication of goodness of fit where the lower is better.\n",
    "\n",
    "$\\frac{1}{n} \\sum |y_i - \\hat{y_i}| $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "y_pred=model2.predict(X_test)\n",
    "print ('MSE: ',mean_squared_error(y_test,y_pred))\n",
    "print ('MAE: ',mean_absolute_error(y_test,y_pred))\n",
    "print ('R2: ', r2_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Are these results sensible? Is the mean absolute error here meaningful? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hyper-Parameter Tuning - Exhaustive Grid Search\n",
    "\n",
    "Two generic approaches to sampling search candidates are provided in scikit-learn: for given values, **GridSearchCV** exhaustively considers all parameter combinations, while **RandomizedSearchCV** can sample a given number of candidates from a parameter space with a specified distribution. \n",
    "\n",
    "```from sklearn.model_selection import GridSearchCV```\n",
    "\n",
    "```class sklearn.model_selection.GridSearchCV(estimator, param_grid, *, scoring=None, n_jobs=None, iid='deprecated', refit=True, cv=None, verbose=0, pre_dispatch='2*n_jobs', error_score=nan, return_train_score=False)```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "model2 = linear_model.Lasso(alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'alpha': [1e-1,1e-2,1e-3,1e-4,1e-5,1e-6]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "### Grid Search cross validation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "LASSO_GridSearch = GridSearchCV(model2, tuned_parameters, cv=2, scoring='r2')\n",
    "LASSO_GridSearch.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print (\"Best Score: {}\".format(LASSO_GridSearch.best_score_))\n",
    "print (\"Best params: {}\".format(LASSO_GridSearch.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 6.1.3 Principal Component Analysis\n",
    "\n",
    "The goal of principal component analysis **PCA** is to find a sparse set of ordered uncorrelated components that are linear combinations of the inputs in capturing most of the variance in the data. It is a dimension reduction technique. The first principal components capture the most variance follow by the second components and so on.\n",
    "\n",
    "<img src=\"pca.png\" alt=\"Description\" width=\"400\" height=\"300\">\n",
    "\n",
    "The technique is used in exploratory data analysis and dimension reduction. Algorithmically this can be accomplished by either applying the Eigendecomposition on the covariance matrix or the Singular Value Decomposition on the data matrix to find these components. PCA is sensitive to the scaling of features. Before applying PCA, we have to scale our data. Each scaled feature will have unit variance. \n",
    "\n",
    "After running principal components analysis, given some input $x_j,(j=1,...,p)$, the first principal component is $pc^1=w^1_1x_1+w^1_2x_2+...+w^1_px_p$ where $w^1_j,(j=1,...,p)$ are the weights learnt from the decomposition.\n",
    "\n",
    "```from sklearn.decomposition import PCA```\n",
    "\n",
    "```class sklearn.decomposition.PCA(n_components=None, *, copy=True, whiten=False, svd_solver='auto', tol=0.0, iterated_power='auto', random_state=None)```\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "scaler = StandardScaler()\n",
    "scaled_x = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#inv_x = scaler.inverse_transform(scaled_x)\n",
    "n=int((scaled_x.shape[1]))\n",
    "pca = PCA(n_components=n)\n",
    "X_proj = pca.fit_transform(scaled_x)\n",
    "pcadf = pd.DataFrame(data = X_proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "response=list(range(n))\n",
    "plt.bar(response, pca.explained_variance_ratio_, color='black')\n",
    "plt.xlabel('PCA features')\n",
    "plt.ylabel('variance %')\n",
    "plt.xticks(response)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "response=list(range(n))\n",
    "pcadf_p=pcadf[response]\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_),color='black')\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Cumulative explained variance')\n",
    "plt.show()\n",
    "np.sum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "From the plot we can see that the first 4 components contributes to 81% of the total variance. We can make a heatmap to see how the original features contribute to create each one of these component.\n",
    "\n",
    "This will tell us how much each feature influences each component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.matshow(pca.components_[:4],cmap='viridis')\n",
    "plt.yticks([0,1,2,3],['1st Comp','2nd Comp','3rd Comp','4th Comp'],fontsize=10)\n",
    "plt.colorbar()\n",
    "plt.xticks(range(len(X.columns)),X.columns,rotation=65,ha='left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 6.1.4 PySal - Spatial Data Science Library in Python \n",
    "\n",
    "For this part of the notebook, we wanted to introduce one more library to our utensil called PySAL that can conduct both exploratory spatial data analysis as well as more advance spatial regression model such as those you will be learning in ```GEOG0114```. \n",
    "\n",
    "PySAL is an open-source project designed to support spatial data science. It was founded as a collaboration between Serge Rey and Luc Anselin in 2005 and since 2018 has been restructured as a meta-package that brings together a family of packages for spatial data science. \n",
    "\n",
    "It has four essential components where libraries sits within each;\n",
    "* Lib - core spatial data structures\n",
    "* Explore - conduct EDA for spatial and temporal data\n",
    "* Model - estimationg of spatial regression models\n",
    "* Viz - visualize patterns in spatial data\n",
    "\n",
    "![pysal](https://pysal.org/pysal1.png)\n",
    "\n",
    ">pip install pysal\n",
    "\n",
    ">conda install -c conda-forge pysal\n",
    "\n",
    "For more details please see:  \n",
    "https://pysal.org/esda/notebooks/spatialautocorrelation.html (new syntax)\n",
    "\n",
    "**Reference** </p>\n",
    "PySAL: A Python Library of Spatial Analytical Methods, Rey, S.J. and L. Anselin, Review of Regional Studies 37, 5-27 2007."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pysal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### from EDA to ESDA (Exploratory Spatial Data Analysis)\n",
    "a family of techniques to explore and characterise spatial patterns in data. There are a series of techniques that places space as a 'first class citizen'. How can ESDA useful?:\n",
    "   * is the variable I'm interested in concentrated in space?\n",
    "   * do similar values correspond to observations at are close to each other in space?\n",
    "   * can I identify any particular areas that have concentrations of similar values?\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Spatial Autocorrelation (Moran's I) - (PSA) \n",
    "\n",
    "You will learn more about this set of method in ```GEOG0114 Principles of Spatial Analysis```. However we believe it is also important to know you can compute similar statistics also in Python. Spatial autocorrelation is a fundamental concept in spatial data science - where things happen has a role in explaning why they happen. \n",
    "\n",
    "    \"Everything is related to everything else, but near things are more related than distant things\" - Waldo Tobler, 1970\n",
    "    \n",
    "**Spatial Autocorrelation** is a statistical represantation of Toblers law, the spatial counterpart of traditional correlation.\n",
    "\n",
    "* Global spatial autocorrelation\n",
    "* Local spatial autocorrelation\n",
    "\n",
    "### Spatial Autocorrelation using PySAL\n",
    "\n",
    "When you install PySal (a meta-package), a number of libraries gets installed. For this short demonstration we would be using the base LibPySAL library and the ESDA Library to conduct spatial autocorrelation on the US Income dataset that gets pre-installed in the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#by convention, we use these shorter two-letter names\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# these two lines uses the pysal libraries\n",
    "import libpysal as lps\n",
    "import esda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Â let's look at the income dataset\n",
    "lps.examples.explain('us_income')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# get both the shapefile and the csv\n",
    "\n",
    "shp_path = lps.examples.get_path('us48.shp')\n",
    "us=gpd.read_file(shp_path)\n",
    "\n",
    "csv_path = lps.examples.get_path('usjoin.csv')\n",
    "df=pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# merged the two dataset\n",
    "us['STATE_FIPS']=us['STATE_FIPS'].astype('int')\n",
    "merged=pd.merge(us,df,left_on='STATE_FIPS',right_on='STATE_FIPS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# visualise the income in the US in 2009\n",
    "merged.plot(column='2009',figsize=(8,8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Types of Neighbours (Spatial Contiguity)\n",
    "\n",
    "![](neighbours.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Â k-nearest neighbours weights for calculating moran's I\n",
    "from libpysal.weights import KNN\n",
    "knn4 = KNN.from_shapefile(shp_path, k=4)\n",
    "\n",
    "# calcualte moran's I for k-nearest neighbour\n",
    "moransI = esda.Moran(merged['2009'], knn4)\n",
    "\n",
    "#Â reporting (null:spatial autocorrelation is significant)\n",
    "print (f'Morans I: {moransI.I}')\n",
    "print (f'Morans I(p-value): {moransI.p_sim}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# queens neighbour weights for calculating moran's I\n",
    "from libpysal.weights import Queen\n",
    "qW = Queen.from_shapefile(shp_path)\n",
    "\n",
    "# calcualte moran's I for Queen's neighbour\n",
    "moransI = esda.Moran(merged['2009'], qW)\n",
    "\n",
    "#Â reporting (null:spatial autocorrelation is significant)\n",
    "print (f'Morans I: {moransI.I}')\n",
    "print (f'Morans I(p-value): {moransI.p_sim}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# rooks neighbour weights for calculating moran's I\n",
    "from libpysal.weights import Rook\n",
    "rW = Rook.from_shapefile(shp_path)\n",
    "\n",
    "# calcualte moran's I for Rook's neighbour\n",
    "moransI = esda.Moran(merged['2009'], rW)\n",
    "\n",
    "#Â reporting (null:spatial autocorrelation is significant)\n",
    "print (f'Morans I: {moransI.I}')\n",
    "print (f'Morans I(p-value): {moransI.p_sim}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Many more advance functions to explore in PySAL\n",
    "\n",
    "There are many many functions in **PySAL** (the library is constantly being updated) that we did not have the bandwidth to cover in this notebook. PySAL has a highly informative website with lots of examples. Some of these can complement your learning of spatial regression in **PSA**. \n",
    "\n",
    "One example of this is the Geographic Weighted Regression or GWR. \n",
    "https://pysal.org/notebooks/model/mgwr/GWR_Georgia_example.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Lab Exercise 6.2 : Supervised Learning - Regression Analysis\n",
    "-------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Exercise 6.2.1: Principal Component Regression\n",
    "\n",
    "**Principal component linear regression**\n",
    "\n",
    "1. conduct principal component linear regression analysis with a progressively increasing number of components, and store the results for each successive iteration. Use PCA features created above (pcadf_p) and create a loop where each iteration use one component more in the training set. tips: np.arange(0,n,1) \n",
    "\n",
    "2. report the r2 and mse for each iteration\n",
    "\n",
    "**SGD Regression**\n",
    "\n",
    "3. use the SGD regression instead where gridsearch is applied to find the optimal learning rate [1,0.5,0.1,0.01,0.001,0.0001] \n",
    "\n",
    "4. plot the coefficients for the optimal learning rate \n",
    "\n",
    "**decision tree regression**\n",
    "\n",
    "5. report the r2 and mse with a decision tree regression (extra: can try other methods)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. conduct principal component linear regression analysis and store the results for each successive components.\n",
    "# tips: np.arange(0,n,1) \n",
    "n=int(pcadf_p.shape[1])\n",
    "var=[] #An empty list to store the feature indices that are used in each iteration\n",
    "r2=[]\n",
    "mse=[]\n",
    "for i in np.arange(0,n,1):\n",
    "    var.append(i)\n",
    "    (X_train, X_test, y_train, y_test) = train_test_split(pcadf_p[var], y, train_size=0.7, random_state=1)\n",
    "    #print (var)\n",
    "    model=LinearRegression()\n",
    "    model.fit(X_train,y_train)\n",
    "    y_pred=model.predict(X_test)\n",
    "    r2.append(r2_score(y_test,y_pred))\n",
    "    mse.append(mean_squared_error(y_test,y_pred))\n",
    "    print (f'{i}th MSE: {mean_squared_error(y_test,y_pred)}')\n",
    "    print (f'{i}th R2: {r2_score(y_test,y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. report the r2 and mse in a plot\n",
    "pd.DataFrame(r2).plot(c='blue')\n",
    "plt.show()\n",
    "pd.DataFrame(mse).plot(c='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Â 3. run SGD regression using gridsearch to find the optimal learning rate [1,0.5,0.1,0.01,0.001,0.0001] \n",
    "from sklearn.linear_model import SGDRegressor\n",
    "n=int(pcadf_p.shape[1])\n",
    "var = np.arange(0, n, 1)\n",
    "\n",
    "# Split the dataset into training and testing sets (70% train, 30% test)\n",
    "(X_train, X_test, y_train, y_test) = train_test_split(pcadf_p[var], y, train_size=0.7, random_state=1)\n",
    "\n",
    "# Initialize the SGDRegressor model with a maximum iteration limit of 1000\n",
    "sgdmodel = SGDRegressor(max_iter=1000)\n",
    "\n",
    "# Define the hyperparameter tuning grid for the learning rate (alpha)\n",
    "tuned_parameters = [{'alpha': [1, 0.5, 0.1, 0.01, 0.001, 0.0001]}]\n",
    "\n",
    "# Set up GridSearchCV to find the best learning rate using R2 as the scoring metric\n",
    "SGD_GridSearch = GridSearchCV(sgdmodel, tuned_parameters, scoring='r2')\n",
    "\n",
    "# Fit the grid search on the training data\n",
    "SGD_GridSearch.fit(X_train, y_train)\n",
    "\n",
    "# Print the best score and the best hyperparameters found\n",
    "print(\"Best Score: {}\".format(SGD_GridSearch.best_score_))\n",
    "print(\"Best params: {}\".format(SGD_GridSearch.best_params_))\n",
    "\n",
    "### 4. plot the coefficients for the optimal learning rate \n",
    "# Initialize a new SGDRegressor with the best learning rate (alpha) found\n",
    "model3 = SGDRegressor(max_iter=1000, alpha=SGD_GridSearch.best_params_['alpha'])\n",
    "\n",
    "# Train the model on the training data\n",
    "model3.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model3.predict(X_test)\n",
    "\n",
    "# Print the Mean Squared Error and RÂ² score for the model on the test set\n",
    "print('MSE: ', mean_squared_error(y_test, y_pred))\n",
    "print('R2: ', r2_score(y_test, y_pred))\n",
    "\n",
    "# Create a DataFrame to visualize the feature importance (coefficients)\n",
    "coef = pd.DataFrame(model3.coef_, index=['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11'])\n",
    "coef = coef.reset_index()\n",
    "coef.columns = ['features', 'importance']\n",
    "\n",
    "# Take the absolute values of the coefficients for comparison\n",
    "coef['importance'] = np.abs(coef['importance'])\n",
    "\n",
    "# Sort the coefficients by importance in descending order\n",
    "coef = coef.sort_values(by='importance', ascending=False)\n",
    "\n",
    "# Plot the feature importances using a barplot\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.barplot(x='importance', y='features', data=coef)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5. report the r2 and mse with a decision tree regression to practice. (extra: can try other methods).\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var=[]\n",
    "NNr2=[] # List to store RÂ² scores for each model iteration\n",
    "NNmse = []  # List to store Mean Squared Errors (MSE) for each model iteration\n",
    "\n",
    "for i in np.arange(0,n,1):\n",
    "    var.append(i)\n",
    "    (X_train, X_test, y_train, y_test) = train_test_split(pcadf_p[var], y, train_size=0.7, random_state=1)\n",
    "    \n",
    "    # Initialise the Decision Tree Regressor with a maximum depth of 12\n",
    "    model = DecisionTreeRegressor(max_depth=12, random_state=1)\n",
    "    # Train the model on the training data\n",
    "    model.fit(X_train, y_train)\n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate and store the RÂ² score for the current iteration\n",
    "    NNr2.append(r2_score(y_test, y_pred))\n",
    "    \n",
    "    # Calculate and store the MSE for the current iteration\n",
    "    NNmse.append(mean_squared_error(y_test, y_pred))\n",
    "    \n",
    "    # Print the MSE and RÂ² for the current iteration\n",
    "    print(f'{i}th MSE: {mean_squared_error(y_test, y_pred)}')\n",
    "    print(f'{i}th RÂ²: {r2_score(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([NNr2,r2],index=['nn','ln']).T.plot()\n",
    "plt.title('r2')\n",
    "\n",
    "pd.DataFrame([NNmse,mse],index=['nn','ln']).T.plot()\n",
    "plt.title('mse')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  },
  "vscode": {
   "interpreter": {
    "hash": "0a1c557b9dec9117a29046a515356257e5802bdb55f3c230e05b4c0616a6393c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
